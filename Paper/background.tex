\subsection{Byte-Pair Encoding}
Byte-Pair Encoding (BPE) was introduced for text compression by Gage (1994) and adapted for neural machine translation subword segmentation by Sennrich et al. (2016). The algorithm starts with a vocabulary of individual characters (or bytes) and iteratively merges the most frequent adjacent pair until a target vocabulary size is reached. At inference time, the learned merge rules are applied greedily, left-to-right, in priority order.

BPE and its variants (WordPiece, Unigram LM) have become the standard tokenization approach for large language models. Most frontier systems from GPT-2 through GPT-5, the LLaMA family, Gemini, and Claude rely on BPE-derived tokenizers.

\subsection{The Abugida Problem}
The fundamental tension between BPE and Abugida scripts has been noted in the multilingual NLP literature. Murikinati et al. (2020) \citep{murikinati2020} highlighted token overhead for Devanagari-based languages in multilingual models. Rust et al. (2021) demonstrated that tokenizer vocabulary allocation for low-resource languages is highly suboptimal in mBERT and XLM-R \citep{rust2021}. Gowda and May (2020) quantified how token fertility correlates with downstream task performance across dozens of languages \citep{gowda2020}.

A critical but under-treated aspect is pre-tokenization. Modern BPE implementations (tiktoken for OpenAI models, the LLaMA regex rules) rely on regular expressions tuned primarily for Latin-script boundaries. These rules frequently fragment multi-codepoint grapheme clusters in Abugida scripts before any statistical merges occur. Variations in Unicode normalization (NFC versus NFD) introduce additional instability in merge priority, a structural sensitivity that purely statistical approaches struggle to mitigate. Consequently, BPE is often limited in its ability to recover linguistically coherent units whose boundaries have already been broken.

The specific pathology of \emph{conjunct fragmentation}---the splitting of ZWJ sequences and virama-mediated consonant stacks---has received less formal treatment. Existing remedies typically retrain a new BPE tokenizer on larger target-language data. These approaches implicitly assume that statistical patterns alone will eventually reassemble fragmented graphemes. This assumption is challenged for scripts where cluster boundaries cannot be reliably inferred from frequency statistics alone.

\subsection{Recent Script-Aware Tokenization Efforts}
Recent work has begun to address Abugida limitations through hybrid designs. Velayuthan et al. (2025) introduced Grapheme Pair Encoding (GPE), operating on Unicode grapheme clusters rather than bytes \citep{velayuthan2025}. Brahma et al. (2025) proposed MorphTok, a morphology-grounded framework with constrained BPE for Hindi and Marathi \citep{brahma2025}. Rana et al. (2025) presented IndicSuperTokenizer, a curriculum-based approach with Indic-tuned regex and NFKC normalization \citep{rana2025}. Sirajudeen et al. (2025) extended LLaMA-3 with Sinhala-specific tokens \citep{sirajudeen2025}.

While these efforts demonstrate meaningful gains in fertility and downstream performance, they remain primarily statistical or hybrid approaches. Few if any incorporate a deterministic, linear-time linguistic pre-processing stage with formal correctness guarantees before statistical compression begins.

\subsection{Rule-Based Syllabification and Pre-segmentation}
Historically, deterministic syllabification algorithms were developed primarily for Text-to-Speech and Optical Character Recognition systems. For instance, Weerasinghe et al. (2005) established foundational rule-based algorithms for Sinhala phonotactics \citep{weerasinghe2005}, and similar finite-state automata have been deployed for Myanmar script segmentation \citep{maung2008, kyawthu2017}. 

Recently, there has been renewed interest in applying these deterministic rules as a pre-processing step for LLMs. Chintha \& Konduru (2025) comprehensively surveyed how subword models fail on Indic ligatures and normalization inconsistencies, underscoring the necessity of script-aware boundaries \citep{chintha2025}. Concurrent hybrid approaches, such as Agathiyam for Tamil \citep{agathiyam2025}, philosophically mirror our approach by layering a deterministic, Sandhi-aware sequence rule before a constrained statistical learner. Alternatively, some models attempt to bypass subword vocabularies altogether using byte-level or character-level encoders like CANINE \citep{clark2022}, though they suffer from vastly increased sequence lengths. Dynamic "Universal" tokenizers have also been proposed to improve language plasticity \citep{abagyan2025}, but as recent studies demonstrate, purely statistical tokenization remains a critical vulnerability that can actively induce reasoning degradation and hallucinations \citep{wang2024}.

\subsection{Sinhala NLP}
Sinhala remains underrepresented in the broader NLP literature. Existing tools include the rule-based word segmenter in the Sinling library (Ranathunga et al., 2019) and several BPE-based tokenizers trained on Sinhala Wikipedia or subsets of the polyglots/MADLAD\_CulturaX\_cleaned collection. Recent work such as SinLlama (Sirajudeen et al., 2025) extends the LLaMA-3 tokenizer vocabulary with Sinhala-specific tokens but does not address conjunct integrity at the architectural level. No prior work has evaluated frontier tokenizers against exhaustive conjunct-dense corpora or proposed a pre-processing stage with formal provable correctness guarantees.