Large Language Models (LLMs) suffer from a fundamental "linguistic blindness" when processing Abugida scripts, imposing a significant "Token Tax" on the Global South. Current tokenization methods, such as Byte-Pair Encoding (BPE), routinely fragment complex conjuncts—multi-codepoint grapheme clusters—into meaningless sub-character units, degrading model reasoning and inflating inference costs. We introduce Syllable-aware Grapheme Pair Encoding (SGPE), a two-layer architecture that strictly decouples linguistic integrity from statistical compression. Layer 1, LinguisTrie, is a deterministic $O(N)$ state machine that segments raw Unicode into atomic syllables and passthrough tokens with a formal zero-breakage guarantee. Layer 2 applies a statistical merge procedure exclusively over this syllable stream, ensuring no token ever violates script structure.
Using Sinhala as a high-complexity proof-of-concept, we trained SGPE on a cleaned 10-million dataset called polyglots/MADLAD\_CulturaX\_cleaned and evaluated it on a held-out 536,508-sentence corpus (59.3 million characters). SGPE achieves a Token-to-Word Ratio (TWR) of \texttt{1.438}—a \texttt{59.1\%} reduction relative to OpenAI’s o200k base tokenizer and \texttt{60.8\%} versus Llama 4—while maintaining perfect structural soundness across all 1,703 enumerated conjunct formations and delivering 4.48 characters per token.
Beyond Sinhala, SGPE establishes a general, scalable O(N) framework for structure-aware tokenization, offering a principled path to reclaim context-window efficiency for more than one billion speakers of Indic and Southeast Asian scripts.