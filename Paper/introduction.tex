The quality of tokenization has an outsized impact on large language model performance. When a tokenizer fragments linguistic units into semantically meaningless sub-units, the model must expend attention capacity on reconstructing structure that should never have been broken. While Byte-Pair Encoding (BPE) and its descendants perform adequately on alphabetic scripts, they encounter fundamental limitations when applied to Abugida writing systems.

Abugida scripts such as Sinhala, Devanagari (Hindi, Sanskrit etc.) are organized around atomic syllables: a base consonant carries an inherent vowel, modified by diacritics (pili) or combined into complex conjuncts via the virama (\texttt{U+0DCA}, \sn{්}) and Zero-Width Joiner (\texttt{U+200D}). Standard script-unaware tokenizers treat these multi-codepoint grapheme clusters as independent sequences, routinely breaking them. For example, common Sinhala conjuncts are often split into many sub-tokens by frontier tokenizers such as OpenAI’s o200k, disrupting both orthographic integrity and semantic coherence.

This fragmentation produces markedly higher token fertility. English typically achieves a token-to-word ratio near 1.3, whereas Sinhala and other Abugida scripts often exceed 4.0 in current models. Because context windows and inference costs are measured in tokens, the disparity—termed the ``Token Tax'' \citep{ahia2023, petrov2023, ali2024}---effectively reduces usable context length for more than a billion speakers of Indic and Southeast Asian scripts by a factor of three to four. This representational ambiguity in turn correlates with degraded model behavior: multiple token sequences map to identical phonetic forms, increasing hallucinations and lowering performance on tasks requiring precise linguistic understanding, including spelling precision, named entity recognition, and semantic consistency \citep{wang2024}.

In this work we introduce \textbf{Syllable-aware Grapheme Pair Encoding (SGPE)}. Our core architectural insight is to embed a strong \emph{structural inductive bias} into the tokenization layer by enforcing linguistic constraints \emph{before} any statistical merging. A deterministic $O(N)$ state machine (\textbf{LinguisTrie}) first segments raw Unicode into well-formed syllables and passthrough characters with a formal zero-breakage guarantee; statistical pair encoding then operates exclusively on this linguistically sound stream. The design rests on a formal characterization of the Sinhala syllable as a regular language, enabling provable correctness and linear-time segmentation. By cleanly decoupling linguistic integrity from statistical compression, \textbf{SGPE} frees the language model from learning basic character reconstruction and restores both linguistic integrity and computational efficiency at the source.