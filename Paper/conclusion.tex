We have presented Syllable-aware Grapheme Pair Encoding (SGPE), a bimodal architecture that strictly decouples linguistic integrity from statistical compression. A deterministic $O(N)$ state machine first segments raw Unicode into well-formed syllables with a provable zero-breakage guarantee; pair merging then operates exclusively over this linguistically sound stream.

On a 59.3-million-character held-out corpus, SGPE achieves a Token-to-Word Ratio of 1.438 — a 59.1\% reduction versus OpenAI’s o200k, 60.8\% versus Llama 4, and 75.8\% versus DeepSeek V3 — while preserving perfect structural soundness. The same architectural pattern generalizes across Abugida scripts and offers a principled path toward tokenizers that are both more efficient and more equitable for the world’s diverse writing systems.