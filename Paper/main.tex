\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{tikz}

% === STYLING ===
\usepackage[margin=1.25in, top=1.25in, bottom=1.5in]{geometry}
\usepackage{mathpazo} % Palatino font 
\usepackage{setspace}
\setstretch{1.05} % Slight line stretch for readability

% === UNICODE + SINHALA SUPPORT ===
\usepackage{fontspec}
\usepackage{polyglossia}

% 1. Set the languages
\setdefaultlanguage{english}
\setotherlanguage{sinhala}

% 2. Set the main Latin (English) font
\setmainfont[Path=./, Scale=1.0, AutoFakeBold=1.5, AutoFakeSlant=0.2]{cmunrm.ttf}

% 3. Set the Sinhala font family
\newfontfamily{\sinhalafont}[Path=./, Script=Sinhala, Scale=1.05, AutoFakeBold=1.5]{NotoSinhala.ttf}

% --- Math Environments ---
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

% Convenient macro â€” use everywhere
\newcommand{\sn}[1]{{\sinhalafont #1}}

\title{The Syllable is the Token: Breaking the Token Tax with SGPE}

\author{
  \textbf{Kusal Darshana} \\
  Remeinium Research \\
  \texttt{thekusaldarshana@remeinium.com}
}

\begin{document}

\maketitle

\begin{abstract}
\input{abstract}
\end{abstract}

\section{Introduction}
\input{introduction}

\section{Background}
\input{background}

\section{A Regular-Language Framework for Abugida Syllables}
\input{syllable_structure}

\section{Architecture}
\input{architecture}

\section{Training}
\input{training}

\section{Experimental Setup}
\input{experiments}

\section{Discussion}
\input{discussion}

\section{Conclusion}
\input{conclusion}

\section*{Acknowledgements}

We thank the Polyglots FYP\footnote{\url{https://huggingface.co/polyglots}} team for releasing the MADLAD\_CulturaX\_cleaned and the open-source community for the foundational tokenization libraries used in our baselines.

The full source code, including the LinguisTrie implementation, training pipeline, and the evaluation harness, is open-sourced at github\footnote{\url{https://github.com/remeinium/SGPE}}. Pre-trained artifacts, including the \texttt{vocab.json}, \texttt{tokenizer.json} and optimized merge rules, are available via Hugging Face\footnote{\url{https://huggingface.co/remeinium/SGPE}}.

\bibliographystyle{plain}
\begin{thebibliography}{15}

\bibitem{gage1994}
Gage, P. (1994).
\newblock A new algorithm for data compression.
\newblock {\em The C Users Journal}, 12(2), 23--38.

\bibitem{sennrich2016}
Sennrich, R., Haddow, B., and Birch, A. (2016).
\newblock Neural machine translation of rare words with subword units.
\newblock In {\em Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics}.

\bibitem{kudugunta2023}
Kudugunta, S., et al. (2023).
\newblock MADLAD-400: A multilingual and document-level large audited dataset.
\newblock {\em arXiv preprint arXiv:2309.04662}.

\bibitem{ranathunga2019}
Ranathunga, S., et al. (2019).
\newblock Sinling: A Sinhala natural language processing toolkit.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing}.

\bibitem{rust2021}
Rust, P., et al. (2021).
\newblock How good is your tokenizer? On the monolingual performance of multilingual language models.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics}.

\bibitem{gowda2020}
Gowda, T. and May, J. (2020).
\newblock Finding the optimal vocabulary size for neural machine translation.
\newblock In {\em Findings of the Association for Computational Linguistics: EMNLP 2020}.

\bibitem{velayuthan2025}
Velayuthan, S., et al. (2025).
\newblock Grapheme Pair Encoding for Indic scripts.
\newblock In {\em Proceedings of COLING 2025}.

\bibitem{brahma2025}
Brahma, M., et al. (2025).
\newblock MorphTok: Morphology-grounded tokenization for Indic languages.
\newblock {\em arXiv preprint arXiv:2504.10335}.

\bibitem{rana2025}
Rana, A., et al. (2025).
\newblock IndicSuperTokenizer: Curriculum-based tokenization for Indic languages.
\newblock {\em arXiv preprint arXiv:2511.03237}.

\bibitem{sirajudeen2025}
Sirajudeen, A., et al. (2025).
\newblock SinLlama: Sinhala-specific tokenizer extension for Llama-3.
\newblock {\em arXiv preprint arXiv:2508.09115}.

\bibitem{ahia2023}
Ahia, O., Kumar, S., Gonen, H., Kasai, J., Mortensen, D.~R., Smith, N.~A., and Tsvetkov, Y. (2023).
\newblock Do all languages cost the same? Tokenization in the era of commercial language models.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 9900--9912.

\bibitem{petrov2023}
Petrov, A., La Malfa, E., Torr, P.~H., and Bibi, A. (2023).
\newblock Language model tokenizers introduce unfairness between languages.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~36.

\bibitem{ali2024}
Ali, M., Fromm, M., Thellmann, K., Rutmann, R., L\"ubbering, M., Leveling, J., Klug, K., Ebert, J., Doll, N., Schulze Buschhoff, J., Jain, C., Weber, A.~A., Jurkschat, L., Abdelwahab, H., John, C., Ortiz Suarez, P., Ostendorff, M., Weinbach, S., Sifa, R., Kesselheim, S., and Flores-Herr, N. (2024).
\newblock Tokenizer choice for LLM training: Negligible or crucial?
\newblock In {\em Findings of the Association for Computational Linguistics: NAACL 2024}, pages 3907--3924.

\bibitem{wang2024}
Wang, D., Li, Y., Jiang, J., Ding, Z., Luo, Z., Jiang, G., Liang, J., and Yang, D. (2024).
\newblock Tokenization matters! Degrading large language models through challenging their tokenization.
\newblock {\em arXiv preprint arXiv:2405.17067}.

\bibitem{agathiyam2025}
Priya, R.~R., Subika, M., Subramanian, V., and Manimegalai, R. (2025).
\newblock Agathiyam: Sandhi-aware tokenization for Tamil language.
\newblock Under review at ICLR 2026.

\bibitem{maung2008}
Maung, Z.~M. and Mikami, Y. (2008).
\newblock A rule-based syllable segmentation of Myanmar text.
\newblock In {\em Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages}.

\bibitem{kyawthu2017}
Thu, Y.~K. (2017).
\newblock sylbreak: Syllable segmentation tool for Myanmar language.
\newblock GitHub repository: \url{https://github.com/ye-kyaw-thu/sylbreak}.

\bibitem{weerasinghe2005}
Weerasinghe, R., Wasala, A., and Liyanage, C. (2005).
\newblock A rule based syllabification algorithm for Sinhala.
\newblock In {\em Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP)}.

\bibitem{chintha2025}
Chintha, S. and Konduru, V. (2025).
\newblock Survey of tokenization mechanisms in multilingual large language models with a focus on Indian languages.
\newblock {\em Journal of Emerging Technologies and Innovative Research}, 12(4).

\bibitem{clark2022}
Clark, J.~H., Garrette, D., Turc, I., and Wieting, J. (2022).
\newblock Canine: Pre-training an efficient tokenization-free encoder for language representation.
\newblock {\em Transactions of the Association for Computational Linguistics}, 10:73--91.

\bibitem{abagyan2025}
Abagyan, A., et al. (2025).
\newblock One tokenizer to rule them all: Emergent language plasticity via multilingual tokenizers.
\newblock {\em arXiv preprint arXiv:2506.10766}.

\end{thebibliography}

\appendix
\input{appendix}

\end{document}
