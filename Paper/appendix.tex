\section*{Appendix A: Sinhala Unicode Coverage}

LinguisTrie handles the complete Sinhala-relevant Unicode block with the following classes:

\begin{table}[h!]
\caption{Sinhala Unicode Coverage in LinguisTrie}
\label{tab:unicode_coverage}
\begin{center}
\begin{tabular}{cll}
\toprule
Range & Count & Description \\
\midrule
U+0D85--U+0D96 & 18 & Independent vowels \\
U+0D9A--U+0DC6 & 45 & Consonants \\
U+0D82--U+0D83 & 2 & Anusvara and visarga \\
U+0DCA & 1 & Virama (HAL) \\
U+0DCF--U+0DDF, U+0DF2--U+0DF3 & 19 & Dependent vowel signs (pili) \\
U+200D & 1 & Zero-Width Joiner \\
\midrule
\textbf{Total} & \textbf{86} & All handled Sinhala-related codepoints \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section*{Appendix B: Token Reduction and Context Extension}

The token reduction of SGPE against a baseline $B$ is
\[
R_B = 1 - \frac{1.438}{\text{TWR}_B}.
\]
The effective context extension factor is
\[
E_B = \frac{\text{TWR}_B}{1.438} = \frac{1}{1-R_B}.
\]
On the 59.3-million-character held-out corpus these yield $R=59.1\%$ ($E\approx2.43\times$) versus OpenAI o200k, $R=60.8\%$ ($E\approx2.55\times$) versus Llama 4, and $R=75.8\%$ ($E\approx4.15\times$) versus DeepSeek V3.

\section*{Appendix C: Zero-Breakage Guarantee Validation}

SGPE was validated on an exhaustive battery of 1,703 distinct conjunct formations that cover every possible combination of virama, ZWJ, dependent vowels, terminal modifiers, and stacking patterns in Sinhala orthography. All 1,703 cases passed with zero violations.

Full-corpus round-trip reconstruction on the 59.3-million-character held-out set produced zero non-UNK mismatches.

\section*{Appendix D: Training Configuration}

\begin{table}[h!]
\caption{SGPE Training Hyperparameters}
\label{tab:hyperparams}
\begin{center}
\begin{tabular}{lc}
\toprule
Parameter & Value \\
\midrule
Target vocabulary size & 100,000 \\
Prune frequency threshold $\theta$ & 100 \\
Minimum merge frequency $f_{\min}$ & 2 \\
Leading-space mode & True \\
Number of workers & CPU count $-1$ \\
Training corpus & 10 M sentences \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section*{Appendix E: Industrial Scalability}

Because Layer 1 (LinguisTrie) operates as a purely deterministic $O(N)$ state machine with no external dependencies or large tensor multiplications, the pre-processing stage imposes minimal computational overhead. 

The Python proof-of-concept presented in this work already processes sentences in fractions of a millisecond on standard CPUs. For industrial, frontier-scale deployment, LinguisTrie can be directly implemented in systems languages such as Rust or C++. Such a port would yield true $O(N)$ linear-time performance at extreme scales, ensuring that the addition of a strict structural inductive bias does not bottleneck high-throughput API endpoints or large-scale pre-training data pipelines.
