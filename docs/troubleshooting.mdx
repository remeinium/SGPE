# Troubleshooting

Common issues and their solutions.

## Installation Issues

### "Import error: No module named 'datasets'"

Install the required packages:

```bash
pip install datasets transformers tiktoken python-dotenv rich tqdm
```

### "Module not found" for local imports

Make sure you're running from the SGPE directory:

```bash
cd SGPE
python -c "from encoder import SGPEEncoder"
```

Or install the package in development mode:

```bash
pip install -e .
```

## Training Issues

### Training is very slow

- Reduce `--num_workers` if you have memory issues
- Increase `--checkpoint_every` to reduce I/O overhead
- Ensure you're using SSD storage for the output directory

### Out of memory

The trainer uses word-type deduplication (storing unique words, not instances). If you still hit memory limits:

- Reduce corpus size
- Increase `--prune_freq` to filter rare syllables earlier
- Use fewer workers: `--num_workers 4`

### "No more pairs found" before reaching vocab_size

This happens when all remaining pairs have frequency below `--min_freq`. Options:

- Lower `--min_freq` (e.g., 1)
- Use a larger or more diverse corpus

### Resume training not working

Check that:
- The checkpoint file exists: `output/checkpoint_*.json`
- You're using the same training file
- Your `prune_freq` and `min_freq` match the original run

## Inference Issues

### Wrong tokenization results

- Verify you're loading the correct `vocab.json`
- Check that `leading_space` mode matches what was used during training

### High UNK rate

This means syllables in your test data weren't seen during training. Solutions:

- Train on more data
- Lower `--prune_freq` during training
- Accept that rare words will produce UNK

### Round-trip failures

If `decode(encode(text)) != text`:

- This is expected if the text contains syllables that were pruned to [UNK]
- Run Battery 4 to check: it should show zero non-UNK mismatches

## Test Suite Issues

### Frontier benchmarking fails to load models

You'll need HuggingFace credentials for some models:

```bash
# Get token from https://huggingface.co/settings/tokens
export HF_TOKEN=your_token_here
```

Models may require accepting licenses (e.g., Llama).

### API tests skipped

Set environment variables:

```bash
export GEMINI_API_KEY=your_google_key
export ANTHROPIC_API_KEY=your_anthropic_key
```

### Test takes too long

- Skip round-trip test: `--skip_roundtrip`
- Use sampled mode (500 sentences) instead of full evaluation
- Reduce test file size

## Data Issues

### Empty lines in output

The cleaner removes lines that become empty after cleaning. This is expected behavior.

### Very high UNK rate after cleaning

Your corpus may have unusual characters. Check with:

```python
from linguis_trie import LinguisTrie

trie = LinguisTrie()
tokens = trie.tokenize(your_text)
print([t for t in tokens if t == "[UNK]"])
```

## Performance

### Slow tokenization speed

SGPE runs at ~30K+ words/second. If slower:

- Ensure you're using the native `SGPEEncoder`, not the HuggingFace wrapper
- Check that your machine has sufficient RAM

### Memory leak during training

The trainer should release memory between phases. If you see growth:

- This is likely due to Python's memory allocator not returning memory to the OS
- Try restarting Python between runs
