# Getting Started

Welcome to SGPE — Syllable-Aware Grapheme Pair Encoding. This guide will help you get up and running with SGPE for Sinhala tokenization.

## Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/remeinium/SGPE.git
cd SGPE
pip install -e .
```

You'll also need these runtime dependencies:

```bash
pip install torch tqdm transformers tiktoken datasets python-dotenv rich
```

## Quick Start

### Using the Pre-trained Tokenizer

The fastest way to get started is to use our pre-trained tokenizer from HuggingFace:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("remeinium/SGPE-Tokenizer")
text = "ශ්‍රී ලංකාව"

tokens = tokenizer.tokenize(text)
print(tokens)
# ['ශ්‍රී', ' ලංකාව']
```

### Using the Native Encoder

For research or custom workflows, use the native `SGPEEncoder`:

```python
from encoder import SGPEEncoder

encoder = SGPEEncoder("output/vocab.json")
ids = encoder.encode("ව්‍යාකරණය")
print(ids)
# [1247, 892]

# Decode back
text = encoder.decode(ids)
print(text)
# "ව්‍යාකරණය"
```

## Directory Structure

Here's what you'll find in the SGPE repository:

```
SGPE/
├── linguis_trie.py      # Layer 1: Syllable segmentation
├── gpe_trainer.py       # Layer 2: BPE training
├── encoder.py           # Inference encoder
├── export.py            # HuggingFace export
├── clean_data.py        # Data preprocessing
├── orchestrator.py      # CLI pipeline tool
├── check_tokens.py      # Interactive tokenizer tester
├── data/
│   ├── download_dataset.py   # Fetch training data
│   ├── train.jsonl          # Training set
│   └── test.jsonl           # Test set
├── tests/
│   ├── battle.py        # Test battery suite
│   └── orchestrator.py  # Interactive test runner
└── output/
    ├── vocab.json       # Trained vocabulary
    └── tokenizer.json   # HuggingFace format
```

## What's Next?

- [Architecture Overview](/docs/architecture) — Understand how SGPE works
- [Training Guide](/docs/training) — Train your own tokenizer
- [API Reference](/docs/api-reference) — Detailed API documentation
- [Testing](/docs/testing) — Run the test suite

## Quick Examples

### Tokenize a Sentence

```python
from encoder import SGPEEncoder

encoder = SGPEEncoder("output/vocab.json")
text = "මම සිංහල භාෂාව ඉගෙන ගන්නවා"

tokens = encoder.tokenize(text)
print(f"Tokens: {tokens}")
# Tokens: [' මම', ' සිංහල', ' භාෂාව', ' ඉගෙන', ' ගන්නවා']

ids = encoder.encode(text)
print(f"IDs: {ids}")
# IDs: [5, 234, 1892, 456, 123, 789]
```

### Compare with Other Tokenizers

```python
import tiktoken
from encoder import SGPEEncoder

sgpe = SGPEEncoder("output/vocab.json")
o200k = tiktoken.get_encoding("o200k_base")

text = "ශ්‍රී ලංකාව රටේ වැඩ කරන අය"

sgpe_tokens = len(sgpe.tokenize(text))
o200k_tokens = len(o200k.encode(text))

print(f"SGPE: {sgpe_tokens} tokens")
print(f"o200k: {o200k_tokens} tokens")
print(f"Reduction: {(1 - sgpe_tokens/o200k_tokens)*100:.1f}%")
```

### Interactive Testing

Use the built-in tokenizer checker:

```bash
# Interactive mode
python check_tokens.py -i

# Check specific words
python check_tokens.py "ශ්‍රී" "ලංකා" "ක්‍රිකට්"

# From a file
python check_tokens.py -f wordlist.txt
```
