# SGPE Documentation

This is the developer documentation for SGPE — Syllable-AAware Grapheme Pair Encoding, a tokenizer specifically designed for Sinhala text.

## Quick Links

- [Getting Started](/docs/nextra/getting-started) — Installation and first steps
- [Architecture](/docs/nextra/architecture) — How SGPE works
- [Training](/docs/nextra/training) — Train your own tokenizer
- [API Reference](/docs/nextra/api-reference) — Detailed API documentation
- [Testing](/docs/nextra/testing) — Test suite and benchmarks
- [Troubleshooting](/docs/nextra/troubleshooting) — Common issues and solutions
- [Contributing](/docs/nextra/contributing) — Extend SGPE for other scripts

## What is SGPE?

SGPE is a two-layer tokenizer that combines linguistic knowledge with statistical learning:

1. **Layer 1 (LinguisTrie)**: A deterministic finite automaton that segments text into atomic syllables. This layer guarantees that no consonant cluster, vowel sign, or modifier is ever split.

2. **Layer 2 (GPE)**: A modified BPE that learns merges on top of the syllable stream, achieving excellent compression while preserving the zero-breakage guarantee.

## Key Features

- **60% token reduction** vs GPT-4o
- **Zero-Breakage Guarantee** — never splits conjuncts or vowel signs
- **O(N) inference** — linear time complexity
- **Glitch-free vocabulary** — no bare viramas or orphan ZWJs

## Project Structure

```
SGPE/
├── linguis_trie.py      # Layer 1: Syllable segmentation
├── gpe_trainer.py       # Layer 2: BPE training  
├── encoder.py           # Inference encoder
├── export.py           # HuggingFace export
├── clean_data.py       # Data preprocessing
├── orchestrator.py     # CLI pipeline tool
├── check_tokens.py     # Interactive tokenizer tester
├── data/
│   ├── download_dataset.py
│   ├── train.jsonl
│   └── test.jsonl
├── tests/
│   ├── battle.py       # Test battery suite
│   └── orchestrator.py
└── output/
    ├── vocab.json
    └── tokenizer.json
```

## License

Apache 2.0 — see LICENSE file for details.

Developed by **Remeinium Research**.
